Добрый день, уважаемые студенты! Я рад вас видеть на очередной лекции. Мы продолжаем предыдущую тему, а именно тему Compute. Сегодня у нас вторая часть. Итак, давайте начнем. В четвертой секции мы с вами поговорим про сервисы, связанные с контейнерами. Далее, в пятой секции, мы поговорим про сервис AWS Lambda. И самая последняя, шестая секция, про сервис AWS Elastic Beanstalk. Секция четвертая, сервисы связанные с контейнерами. Перед тем как мы доберемся до контейнеров, давайте вспомним. У нас есть физический сервер. Внутри дата-центров AWS физически на определенном участке расположены максимально мощные физические сервера. Далее, благодаря виртуализации, вот этот физический виртуальный сервер подразделяется, делится на независимые друг от друга виртуальные инстанции EC2. Таким образом, в рамках EC2 мы можем запрашивать более 50 различных типов, отличающихся количеством процессоров ядер процессора, а также количеством оперативной памяти. Теперь, двигаясь еще дальше, есть еще один вариант виртуализации на уровне операционной системы, когда мы устанавливаем операционную систему и поверх операционной системы запускаем изолированный контейнер. То есть, если говорим про EC2 Instance, там вы указываете во время создания определенную операционную систему. Когда мы говорим про контейнер, контейнер привязан к определенной операционной системе и запускается в рамках этой операционной системы. Но все остальное, а именно библиотеки, все конфигурации, код, рантайм, среда запуска вашей программы, она вся изолирована и самодостаточная. Таким образом, мы говорим, что контейнер это что-то, что repeatable, то есть, оно легко переносимое. То есть, вы берете готовый контейнер и копируете ее, запускаете в другом месте, в другом инвариументе, и она будет запускаться точно так же, так как она содержит все необходимое для своего функционирования. Еще другой важный момент, это то, что виртуальные машины запускаются намного медленнее по сравнению с контейнерами. Теперь двигаемся дальше. Для того, чтобы создавать контейнеры, работать с ней, нам нужна определенная программа. Самая популярная программа, которая позволяет нам создавать контейнеры, это Docker. Docker-контейнер. Docker-контейнер, это такая сущность, которая содержит в себе все необходимое для запуска и корректного функционирования вашего приложения, а именно библиотеки, системные инструменты, код и среда запуска вашей программы. Мы проговорили, что такое контейнер самыми простыми словами. Теперь давайте копнем немножко глубже. Когда мы говорим про виртуальные машины, с правой стороны вы видите пример деплоймента трех различных приложений, которые работают с различными библиотеками. Они специально выделены различным цветом, то есть application1, application2, application3. Каждая из них запущена на инстанции EC2. При этом вы видите, что операционная система, она изолирована друг от друга. То есть VM1, виртуал машин первая, у нее своя операционная система, она может совпадать с другими, а может и нет. Это зависит от самого приложения. То есть виртуальная машина работает поверх гипервизора. Теперь следующий уровень виртуализации это контейнеры. То есть выше гипервизора у нас уровень операционной системы нашего EC2 инстанца. Так вот в рамках одного инстанца EC2 вы видите с левой стороны, что мы запустили три различных контейнера. Каждый контейнер в себе содержит не только приложение, но и все, что необходимо для корректного функционирования каждого приложения. И при этом вы видите, что приложения работают с различными библиотеками, не связанные между собой. Самое главное, что была нужная операционная система. Здесь необходимо также сделать пометку, что контейнеры, они достаточно гибкие и могут запускаться на различных операционных системах. Основное требование, чтобы были все реализованы, все необходимые фичи, функционал, который нужно для конкретного контейнера. У нас есть различные варианты операционной системы Linux. В том случае, когда для нашего контейнера нужен тот функционал, который есть у двух различных вариантов Linux-операционной системы, мы говорим о том, что этот контейнер может успешно запуститься и в первом, и во втором варианте операционной системы. Таким образом, контейнер в том же состоянии, который есть, вы можете запускать на любом другом компьютере на отличающейся операционной системе. И это придает некоторую гибкость во время работы с контейнерами. Другой момент, это то, что в рамках одного из этого контейнера мы можем запускать очень маленькие контейнеры и в одном и ситу инстанции может находиться в один момент времени сотни различных контейнеров. Каждый контейнер обслуживает какое-то определенное приложение. Теперь двигаясь к AWS, вы скорее всего уже в уме запланировали, что для того, чтобы работать с контейнерами, вам необходимо запустить один и ситу инстанции, там установить докер приложения и вы можете работать с контейнерами. Да, это правильный вариант работы с контейнерами в облаке, но не самый лучший. Самый лучший вариант, это есть специально созданный для этого сервис Amazon Elastic Container Service. Чаще она встречается в сокращенном варианте EACS и дает возможность вам работать с контейнерами в более удобном виде. Таким образом вы можете запускать до 10 тысяч ваших контейнеров за несколько секунд, используя этот сервис. Вы можете также удобно мониторить, управлять и даже настраивать различные действия и расписания для запуска и управления вашими контейнерами. Здесь также следует отметить, что EACS поддерживает не только on-demand, e-situ-instances, а также spot-instances и reserved-instances. Давайте рассмотрим пример. У нас есть некий task definition. Task definition это описание вашего контейнера. В нем содержится информация о вашем приложении, какие порты вы используете, дополнительные параметры возможно вы задаете для работы приложения. И представим, что в этом примере мы сделали task definition, описали два контейнера, контейнер A и контейнер B. Теперь в рамках нашей инфраструктуры нам необходимо три инстанца контейнера A и два инстанца контейнера B. Таким образом мы делаем task, то есть задача, либо instance контейнера, если так можно выразиться. Таким образом мы вот эти маленькие инстанции, в которых сидят наши контейнеры, через task передаем сервис Amazon EACS. И этот сервис для нас в зависимости от наших дополнительных входных данных для сервиса EACS располагает эти контейнеры и запускает их в нашем кластере EACS. Кластер EACS это набор EC2 инстанцев, которые запущены в виде группы. Таким образом сервис Amazon EACS устанавливает на каждом EC2 инстанции агент. Этот агент как раз таки помогает вам располагать ваши контейнеры внутри этих EC2 инстанцев. Когда мы говорим про ECS кластер для нас доступны три варианта. Первый вариант это когда мы создаем наши контейнеры и описываем дополнительные параметры какой мощности должна быть этот контейнер, в каком количестве мы эти контейнеры создаем, а также необходимые параметры для настройки сети и взаимодействия этих контейнеров между собой, то есть networking related settings. В этом случае мы говорим про кейс с правой стороны и благодаря связке сервисов Amazon ECS и сервиса AWS Fargate мы можем сконцентрироваться лишь на наших контейнерах. Все остальное, то есть управление операционной системы, управление докер-агентом, также докер-приложением, где запускаются и работают наши контейнеры, оно передается под управление AWS. Это в случае если у вас нет ресурсов либо специалистов или нет необходимости какой-то супер тонкой настройки ваших EC2 инстанцев, где запущены ваши контейнеры. В случае же если вам нужна какая-то тонкая настройка подвинутая, она не всегда необходима. Но в этом случае вы можете воспользоваться другим вариантом слева, когда вы полностью контролируете и инстанции в рамках ECS-кластера и указываете все необходимые более тонкие настройки. В этом случае есть два варианта. Ваш кластер ECS-кластер будет состоять из EC2-инстанцев либо Linux, либо Windows. И в том и в этом случае вам необходимо будет помимо того, что вы делаете в первом варианте, вам также необходимо ввести все настройки, которые необходимы для создания одного EC2-инстанца. Используя этот шаблон ваших входных данных, уже будет создаваться ECS-кластер. Давайте представим следующий кейс. Вы компания, разрабатываете некоторое приложение и используете докер контейнеры для этого. Компания растет, через некоторое время количество контейнеров, которые у вас есть, оно увеличивается. И вам нужно что-то, что помогло бы вам эффективно оркестрировать, то есть управлять вот этот большой объем ваших контейнеров. Для этого было создано опенсорсное приложение специально для оркестрации контейнеров, называется Kubernetes. Либо вы можете чаще встречать как K8S. Чтобы понять, что такое Kubernetes, когда мы говорим про докер, мы говорим, что как гостевая операционная система, мы работаем в рамках одной гостевой операционной системы. Когда мы говорим про Kubernetes, мы уже поднимаемся на уровень выше и работаем уже с несколькими докер хастами. Kubernetes нам позволяет упростить задачи масштабирования, провиженинга, то есть запуска наших контейнеров, вопросы связанные с настройкой сетей, а также распределением нагрузки. То есть это то решение, которое нам помогает эффективно справляться с этим большим количеством разнородных контейнеров. Когда мы говорим про Kubernetes, также есть некоторые популярные термины. Kubernetes управляет кластером. Кластер – это набор нескольких виртуальных машин. Каждая виртуальная машина в рамках кластера называется нодой. Мы с вами помним, что в рамках одной виртуальной машины может запускаться несколько сотен контейнеров. Так вот, в нашем случае, в случае Kubernetes, как контейнер выступает подой. То есть таким образом у нас кластер состоит из нодов, и в каждой ноде может запускаться большое количество подов, то есть контейнеров. Мы двигаемся дальше. Теперь поговорим про следующий сервис. Вы, наверное, подумали, что вы сейчас можете поднять кластер Kubernetes следующим образом. Запускаете несколько issue2-инстанцев, устанавливаете docker-приложение, поверх устанавливаете Kubernetes-приложение, и вот у вас есть Kubernetes-кластер, с которым вы можете работать. Да, это действительно так, это один из вариантов, но когда мы работаем с AWS, у нас есть вариант получше, а именно сервис Amazon Elastic Kubernetes Service. Чаще вы его будете встречать как Amazon IKS. Это тот сервис, который предоставляет нам Managed Kubernetes Service. То есть это тот сервис, который полностью совместим с приложением Kubernetes, и сервис позволяет пользователям, клиентам нашего облачного провайдера AWS разгрузиться, и большую часть операционной деятельности по обслуживанию этого кластера передать AWS, а сами больше сконцентрироваться на бизнес-задачах. То, что этот сервис совместим с приложением Kubernetes, говорит о том, что мы наши уже запущенные на локальном дата-центре нагрузки на Kubernetes можем с легкостью перенести на AWS, а именно в сервис Elastic Kubernetes Service. У вас может возникнуть вопрос, чем же все-таки отличается сервис Amazon ECS, а также Amazon IKS. На самом деле оба варианта, оба сервиса помогают нам оркестрировать нашим кластером Docker-контейнеров. Отличие лишь в том, что в первом случае мы работаем непосредственно с Docker-контейнерами и управляем нашим кластером посредством сервиса AWS. Когда мы говорим про IKS, оркестрации кластера уже занимается не решением от AWS, а программа опенсорсная Kubernetes, но она обернута в сервис, который нас разгружает от некоторых операционных моментов. То есть они похожи в зависимости от того, что у вас установлено, как ваша инфраструктура поднята, как она функционирует, либо если сейчас вы ни то ни другое не используете, то как минимум у клиентов AWS есть возможность выбрать один из этих вариантов. Следующий сервис, о котором я хотел вам рассказать, это Amazon Elastic Container Registry. Чаще вы его будете встречать как Amazon ECR. Это тот сервис, который является хранилищем всех ваших Docker-образов. Таким образом, когда вы создаете ваш кластер, неважно с использованием сервиса Amazon ECS, либо с использованием сервиса Amazon IKS, вам необходимо указать ваш контейнер, образ этого контейнера. В этом случае необходимо воспользоваться сервисом Amazon ECR. Как аналогия могу привести, когда мы с вами создаем EC2-инстанц, мы указываем AMI, то есть образ вашего будущего инстанца. AMI в этом случае выступает как Docker-образ. И хранилище всех ваших AMI в случае с контейнерами это сервис Amazon ECR. Мы с вами добрались до конца четвертой секции. Давайте пройдемся по самым основным моментам. Контейнеры это нечто, что может в себе хранить все необходимое для успешного запуска вашего приложения. Сюда входят библиотеки, системные настройки, код и так далее. Docker это та программа, которая позволяет вам создавать контейнеры. Это одна из самых популярных программ для этого. Одно приложение может запускаться в нескольких контейнерах, связанных между собой. Есть сервис Elastic Container Service, то есть ECR, которое позволяет вам оркестрировать вашими Docker-контейнерами. Следующее популярное приложение программы это Kubernetes. Это опенсорсное решение, которое позволяет вам оркестрировать ваши контейнеры. Специально для Кубернетес был создан отдельный сервис Elastic Kubernetes Service. Он совместим с Кубернетес и позволяет вам разгрузиться от операционной работы, но при этом управлять вашим кластером Docker-контейнеров через Кубернетес. И третий сервис это Elastic Container Registry. Это сервис, который является хранилищем всех ваших Docker-контейнеров. Мы с вами добрались до пятой секции и здесь мы поговорим про сервис AWS Lambda. Это мой самый любимый сервис внутри всех сервисов AWS. И сейчас вы узнаете почему. Мы с вами ранее проговаривали, что у нас есть различные сервисы, которые предоставляют различные IT-ресурсы. Когда мы говорим про Compute, EC2-сервис предоставляет нам виртуальные машины. Сервисы ECS и EKS помогают нам работать с контейнерами. Так вот, следующий уровень это AWS Lambda Serverless Computing. То есть, это сервис, который исключает абсолютно все операционные задачи под вас и достаточно вам ваш код загрузить в этот сервис. Все что связано с запуском этого сервиса, поддержкой, так далее, настройкой мониторинга занимается и берет на себя AWS. Таким образом, я как разработчик могу исключить необходимость системного администратора при построении какого-то решения. Также и вы, так как IT-университеты больше выпускают разработчиков, нежели системных администраторов, вы также можете воспользоваться этим сервисом для запуска своих собственных решений, собственных стартап-проектов, где благодаря AWS исключается необходимость целого человека, специалиста, то есть, системного администратора. Это поможет вам сократить некоторые расходы и увеличить шансы успешного запуска вашего стартапа. Касательно оплаты, здесь тоже очень важный момент. Вы оплачиваете за количество времени, когда лямбда была запущена. В случае, когда ваш код не запускается, вы абсолютно ничего не оплачиваете. Это также идеально подходящая модель оплаты для тех же стартапов, либо для тех нагрузок, которые не постоянны. Представьте, вы некий стартап, у вас нагрузки небольшие, потому что вы не наработали клиентскую базу, вы не настолько популярны, таким образом ваш код будет запускаться только в тот момент, когда ваше приложение будут использовать. Если ваше приложение никто не использует, то вы соответственно ничего не оплачиваете. Давайте рассмотрим подробнее, какие преимущества у сервиса AWS LAMBDA есть. С лямбдой вам нет необходимости изучать новые языки программирования. Оно и так изначально поддерживает самые популярные языки программирования, в том числе Python, JavaScript, Node.js, Ruby, C-sharp и так далее. Вы уже поняли, что большую часть администрирования по запуску вашей лямбды берет на себя AWS, вам необходимо лишь загрузить ваш код. Внутри AWS также сидит встроена защита от падений. Что это значит? Это значит, микроконтейнеры, в котором запускается ваш код, находятся в нескольких availability зонах в рамках региона. Таким образом, если что-то происходит в availability зоне, то ваш контейнер запустится в другой availability зоне. Вы этого даже никак не заметите и не почувствуете, не узнаете, оно встроенно сидит внутри лямбды. Таким образом, все ваши запросы на лямбду будут успешно отработаны. Бывают некоторые нагрузки, когда вам необходимо запускать несколько лямбд, это какая-то сложная логика и одна лямбда может запускать другую лямбду, либо в зависимости от результатов выполнения одной лямбды, при некоторых дополнительных условиях вы можете запускать или не запускать другие лямбды. Для построения этой сложной логики, либо аркистрации нескольких лямбда функций, вы можете воспользоваться сервисом AWS Step Functions. Step Functions как раз таки расписывает эту логику и в зависимости от логики она будет вызывать ту или иную лямбду. Но лямбда также может использоваться отдельно, самостоятельно, независимая функция, которая в зависимости от запроса возвращает какой-то готовый ответ. Например, у вас есть веб-сайт и вы запрашиваете информацию о каком-то продукте. Таким образом, когда запрос прилетает до вашей лямбды, лямбда, используя входные данные, идет в хранилище информацию о ваших продуктах и извлекает необходимую информацию, далее возвращает это в вызывающей стороне. Как хранилище у вас может выступить объектное хранилище в качестве сервиса AWS S3. Если у вас SQL база данных, это сервис AWS RDS, в случае если у вас noSQL база данных, то у нас есть AWS DynamoDB, то есть вариантов интеграции сервиса лямбда с другими сервисами имеется большое количество. Также немаловажное преимущество это pay per use pricing, то есть вы оплачиваете только за то время, когда ваша лямбда была запущена. Это позволяет значительно сэкономить ваши затраты, в случае если ваши нагрузки не постоянны. Давайте поговорим о самых популярных сервисах, которые работают в связке с AWS LAMBDA. Когда мы говорим про объектное хранилище, самым популярным вариантом является Amazon S3. Оно нативно интегрируется с этим сервисом и никаких проблем не возникает. Другой вариант, когда мы говорим, что нам необходимо хранить некоторые данные в не структурированном виде, нам необходимо использовать noSQL баз данных. В этом случае выступает Amazon DynamoDB. Amazon DynamoDB является также сервер-лесс решением баз данных noSQL. Таким образом она также нативно интегрирована с сервисом AWS LAMBDA. Далее у нас есть два сервиса AWS SNS, а также Amazon SQS. SNS это когда мы в зависимости от тех или иных условий можем в формате push отправлять уведомления. Когда мы говорим SQS, это работа с очередью и нагрузки формата pull. То есть в случае SNS мы передаем какое-то сообщение, а оно сразу отправляется, пушится к получателям. Когда мы говорим SQS, это pull. То есть в SQS в очереди накапливается определенное количество сообщений и мы в режиме pull, то есть подтягиваем необходимый объем сообщений для обработки в тот момент, когда нам это нужно. Это два варианта построения архитектуры в облаке и оба варианта также интегрированы с AWS LAMBDA. Когда мы говорим, что нам необходимо построить API, то есть Application Program Interface, мы можем интегрировать LAMBDA с Amazon API Gateway. Это одна из самых популярных связок LAMBDA. Еще один существующий, но менее популярный вариант сервиса это Application Load Balancer. Например, может выступать, представьте, у вас есть кластер EC2, он запускается, делает некоторые нагрузки, но при этом вы знаете, что после восьми часов вечера вам смело можно отключать 50% ваших EC2 инстанцев, потому что нагрузка резко уменьшается. В этом случае вы можете настроить Event, которое триггерит вашу LAMBDA в 8 часов вечера, дальше LAMBDA уже передает запрос на сервис Load Balancer, который сокращает количество инстанцев EC2. Давайте теперь подробнее поговорим о том, какие настройки необходимо произвести для запуска нашей LAMBDA функции. Самое первое, нам нужен код программы, которая будет запускаться. Далее нам необходимо настроить IAM role для LAMBDA функции. Если вы создаете LAMBDA функцию в AWS Management Console, то для вас создается роль с минимальными правами, а именно с правами для сервиса CloudWatch, чтобы функция могла писать туда свои логи отработки. Если вам необходимо, чтобы LAMBDA функция работала с другими AWS сервисами, вам необходимо соответствующие полисы, права, доступы добавить в эту роль. Также нам необходимо указать runtime, то есть в какой среде запускать ваш код, либо это Python, Node.js, C-Sharp и так далее. Помимо самого кода LAMBDA, мы также можем использовать другие зависимости. Это могут быть дополнительные кастомные библиотеки, необходимые для запуска вашего кода. В этом случае вам необходимо создать архив и этот архив загружать в сервисы AWS LAMBDA. В случае, когда вы не используете дополнительные библиотеки, библиотеки достаточно редко используются и в случае Python, стандартных библиотек Python хватает для большинства задач. В этом случае у нас есть только код программы и в этом случае вы можете работать в консоли AWS LAMBDA и копировать, вставлять код и сохранять этот код вот в этой же странице, что очень удобно. Касательно времени запуска, функция максимум может запускаться 15 минут, то есть 900 секунд. Больше этого она запускаться не может. В случае, если вам нужно больше времени, вам необходимо рассмотреть вариант использования Step Functions, когда одна LAMBDA запускает следующую LAMBDA, таким образом вы получаете дополнительные 15 минут запуска. В случае, если этот вариант вам не подходит, вы можете также посмотреть в сторону контейнеров либо в сторону EC2-инстанцев. В зависимости от вашей бизнес-задачи тот или иной вариант может быть вам ближе либо выгоден с точки зрения реализации. Когда мы говорим о мощности нашей функции, то мы контролируем количество оперативной памяти, которую мы можем ей выделить. Мы можем выдать минимально 128 МБ для этой функции, либо до 10 ГБ оперативной памяти. Количество ядер будет увеличиваться пропорционально увеличению количества оперативной памяти. Ее мы отдельно от оперативной памяти устанавливать не можем. Давайте рассмотрим пример использования AWS LAMBDA, достаточно популярный пример. У нас есть наборы EC2-инстанцев и в зависимости от наших нагрузок мы понимаем, что в 10 часов вечера мы можем отключать все наши инстанции, а в 5 часов утра нам необходимо стартовать наши инстанции. Как мы можем настроить нашу архитектуру для того, чтобы это все работало. Мы создаем два ивента. Эти ивенты создаются в сервисе AWS CloudWatch. Первый ивент по расписанию в 10 часов вечера будет триггерить первую LAMBDA. Эта LAMBDA на вход принимает этот ивент. Также в этой LAMBDA либо в ивенте мы можем прописать список инстанцев на отключение и в момент запуска этой LAMBDA у нее есть соответствующая роль. У этой роли есть права для отключения из этого инстанцев. Она запускает соответствующие команды и наши истинсы отключаются. Второй ивент он по расписанию триггерит в 5 часов утра уже другую вторую LAMBDA. У этой LAMBDA есть соответствующая либо та же роль, либо другая роль, у которой есть права на запуск истинсов. Так вот в момент триггера ивентом CloudWatch нашей этой LAMBDA функции она исполняет свой код и в зависимости от входных данных списка EC2 инстанцев эти инстанцы она стартует. Давайте теперь рассмотрим второй пример. Представим, что у нас есть некоторое приложение, которое в себе содержит фотографии. Наш пользователь приложения загружает некоторую фотографию. Эта фотография у нас сохраняется в S3 bucket. Далее как только у нас файл загрузился в S3 bucket, происходит запускается ивент, который триггерит нашу LAMBDA. Запускается наша LAMBDA, в ивенте сидят входные данные, а в том числе информация с ссылкой на эту картинку. Мы понимаем какая картинка была загружена. Далее LAMBDA использует свою роль, у нее есть соответствующие права. Она загружает эту картинку, обрабатывает ее и создает эту же картинку в различных разрешениях. В том числе и картинку так называемую thumbnail. Это мини версия этой картинки, которая весит очень мало, но при этом эта картинка отображается, когда мы видим наши картинки в списке. И далее после отработки, на подготовке всех необходимых картинок, она загружает уже эти картинки в другой S3 bucket. И если додумывать дальше, то приложение уже может отображать другим пользователям список этих фотографий в различных разрешениях, а также может использовать thumbnail для отображения мини версии при просмотре списка всех наших картинок. Давайте подробнее остановимся на основных лимитах сервиса AWS Lambda. Здесь хотел бы также отметить, большинство лимитов, она является soft и может быть увеличена по обращению в AWS support. Вы можете в рамках одного региона запускать параллельно до 1000 execution of Lambda. То есть представим, что у вас есть определенная функция, она передает информацию о каком-то продукте и в случае, если внезапно 1000 пользователей запросят информацию о некотором продукте и вам необходимо параллельно запустить 1000 Lambda, то AWS на своей стороне автоматически создаст микроконтейнеры, где запускаются ваши лямбы в необходимом количестве до 1000 и успешно запустит все эти лямбы. Далее, если количество запросов резко уменьшится, то все запущенные микроконтейнеры, они будут постепенно уничтожаться по внутренней логике AWS. В случае, если у вас придет больше запросов, то вы можете настроить retry логику и ваш запрос будет через некоторое время отправлен еще раз. Так как на тот момент уже будет скорее всего запускаться меньшее количество лямб, то эти запросы тоже будут успешно обработаны. Если мы говорим про размер нашей функции, включая код, а также все необходимые библиотеки для запуска, то оно может быть размером до 250 мегабайтов в не архивированном состоянии. Если мы говорим про мощность лямбы, то мы можем выделить минимум 128 мегабайтов оперативной памяти и максимум 10 гигабайтов оперативной памяти для одной нашей функции. Здесь следует отметить, что количество ядер процессора будет выделяться параллельно количеству выделенных мегабайтов оперативной памяти. Если мы говорим про максимальную длительность запуска одной функции, то это 900 секунд либо 15 минут. Мы с вами добрались до конца пятой секции. Давайте пройдемся по самым основным моментам. AWS Lambda это тот сервис, который предоставляет нам бессерверные мощности. Идея в том, что мы не менеджим наши серверы, а лишь загружаем наш код. Все, что относится к обслуживанию запуска этого кода, оно происходит на стороне AWS. Также AWS Lambda включает автоматическое масштабирование. То есть, если внезапно происходит всплеск запросов на нашу лямду, то все эти запросы параллельно будут обработаны. Lambda может быть достаточно мощной и мы можем выделять до 10 гигабайтов оперативной памяти для одной нашей функции. Более того, каждая наша лямда может запускаться максимум 15 минут. Этого времени более чем достаточно для выполнения большинства задач. На этом мы остановились на самых основных моментах, связанных с сервисом AWS Lambda. Я очень надеюсь, что этот сервис вам пригодится в будущем при построении ваших стартапов. Это идеальный выбор для запуска чего-либо в облаке. И также никогда не забывайте, что нельзя сильно привязываться к лямде. И бывают ситуации, когда нужно ваши нагрузки, которые стали постоянными, переносить уже на другие мощности. Но когда мы говорим про самое-самое начало, когда ничего не понятно, ничего не известно, то все серверless лямды, начиная от лямды, заканчивая другими сервисами, работающими в связке с этой лямдой, являются идеальным решением. На этом мы с вами добрались до шестой секции. Последняя секция в рамках нашей сегодняшней лекции. И мы с вами поговорим подробнее про сервис AWS Elastic Beanstalk. Elastic Beanstalk является следующим примером компьют-сервиса, предоставляется в форме PaaS, то есть Platform as a Service. Идея в том, что вы загружаете код вашего веб-приложения, Elastic Beanstalk поднимает всю необходимую инфраструктуру для того, чтобы запустить это веб-приложение. Что подразумевается под поднять инфраструктуру, это фактически развертывание вашего веб-приложения, настройка балансирования, нагрузки, а также автоматического масштабирования и все необходимое связанное с мониторингом и логированием вашего веб-приложения. Здесь следует отметить, что этот сервис, использование этого сервиса является бесплатным, то есть вы ни доллара не оплачиваете за этот сервис. Но все те ресурсы, которые были подняты в рамках сервиса Elastic Beanstalk, они оплачиваются по стандартным тарифам. Например, если вы передали некоторые входные параметры в сервис Elastic Beanstalk и для вас была поднята инфраструктура с двумя EC2-инстанциями, то вы будете оплачивать за эти два EC2-инстанца по стандартному тарифу этого сервиса. Это и относится ко всем другим IT-ресурсам и другим сервисам AWS, с которым взаимодействует Elastic Beanstalk. То есть вы бы столько же денег заплатили бы, если бы вы поднимали те же два EC2-инстанца вручную. Разница лишь в том, что поднятие инфраструктуры было проделано вместо вас, для вас, автоматически через сервис Elastic Beanstalk. Фактически вы оплачиваете ту же сумму, плюс ко всему этому экономите свое время и поднимаете инфраструктуру автоматически. Здесь вы можете наглядно видеть, какую часть работы AWS через сервис Elastic Beanstalk берет на себя. То есть то, что вы управляете, это ваш код и ваши настройки для Elastic Beanstalk. Все остальное управляется AWS, то есть HTTP-сервер, application server, language interpreter, то есть среда запуска вашего кода, операционная система и даже хост. Elastic Beanstalk вы можете использовать как в AWS Management консоли, также вы можете использовать AWS CLI для запуска инфраструктуры с использованием Elastic Beanstalk. Когда мы говорим про поддержку платформ, то поддерживаются Docker, Go, Java,.NET, Node.js, PHP, Python и Ruby и другие платформы. Если мы говорим про веб-серверы, которые поддерживаются, то для Java приложений поддерживается Apache Tomcat, для PHP и Python приложения поддерживается Apache HTTP Server. Если мы говорим про Node.js приложения, для них доступны NGINX и Apache HTTP Server. Если Ruby приложения, то это Passenger и Puma. Если мы говорим про.NET приложения, Java и Docker вместе с Go, то поддерживается Microsoft Internet Information Services, то есть IIS. Давайте остановимся на основных преимуществах сервиса AWS Elastic Beanstalk. Первое это то, что вы очень быстро и просто можете начать, то есть запустить ваше веб-приложение, поднимается вся необходимая инфраструктура, вы фактически меньше времени теряете на все это дело. Другой момент это то, что ваши специалисты, ваши разработчики, они освобождаются от операционной деятельности и могут сконцентрироваться на бизнес-содачах. Таким образом, эффективность ваших разработчиков повышается. Другой момент это то, что Elastic Beanstalk, оно подходит для большинства веб-приложений. Таким образом, очень редко, когда возможности Elastic Beanstalk для веб-приложения становятся мало. И в большинстве случаев вы будете двигаться с Elastic Beanstalk и все будет хватать. Другой момент это то, что в Elastic Beanstalk вы можете достаточно гибко настроить ваши ресурсы. Как пример, вы можете указать определенный instance type в рамках сервиса Amazon EC2, который может поднимать Elastic Beanstalk. На этом мы подошли к концу шестой секции. Давайте остановимся на основных моментах. В случае, когда у вас веб-приложение, то обязательно следует рассмотреть сервис AWS Elastic Beanstalk, так как она помогает вам упростить процесс развертывания вашей IT-инфраструктуры в облаке. Elastic Beanstalk поддерживает достаточно большой выбор платформ — это Java,.NET, PHP, Node.js, Python, Ruby, Go, Docker и другие. Если мы говорим касательно оплаты, это сервис полностью бесплатный для клиентов AWS и вы не оплачиваете за использование этого сервиса. Но вам необходимо оплачивать по стандартным тарифам за те сервисы, где вы создаете ее ресурсы. Например, если Elastic Beanstalk поднимает EC2-инстанции, то да, вы за эти EC2-инстанции в рамках стандартных тарифов будете оплачивать за использование. На этом мы подошли к концу нашей сессии. Давайте остановимся на самых основных моментах. За эти две лекции мы с вами рассмотрели, какие сервисы вычисления для нас доступны в AWS. Мы подробнее познакомились с сервисом Amazon EC2. Далее мы посмотрели, какие сервисы есть, когда мы работаем с контейнерами. Это Amazon EKS, Amazon ECS и Amazon ECR. Далее познакомились с сервер-лесс решением. Это AWS Lambda. Рассмотрели, какие плюсы есть, минусы. Ну и в самом конце познакомились с сервисом для быстрого развертывания в веб-приложении. Это AWS Elastic Beanstalk. Вы видите, что AWS предоставляет широкий выбор сервисов и в зависимости от вашей бизнес потребности для вас могут подойти один или несколько вариантов, которые вы будете использовать. Оно будет максимально эффективно и максимально выгодно для вас. Если вам необходима дополнительная информация по тому или иному сервису, здесь вы видите дополнительные ссылки, которые вам могут быть полезны. На этом мы завершаем наше сегодняшнее лекционное занятие. Я надеюсь, вы научились чему-то новому и увидимся с вами на следующих наших активностях.
